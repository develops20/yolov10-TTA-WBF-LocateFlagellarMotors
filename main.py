{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91249,"databundleVersionId":11294684,"sourceType":"competition"},{"sourceId":11932060,"sourceType":"datasetVersion","datasetId":7501747},{"sourceId":327336,"sourceType":"modelInstanceVersion","modelInstanceId":274744,"modelId":295634}],"dockerImageVersionId":30919,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !tar xfz /kaggle/input/ultralytics-for-offline-install/archive.tar.gz\n# !pip install --no-index --find-links=./packages -q ultralytics\n# !rm -rf ./packages\n# print(\"package installed ...........................\")\n\n!cp -r /kaggle/input/mhafyolo/pytorch/default/1/MHAF-YOLO-main /kaggle/working/\nprint('PIP INSTALL OK!!!')\n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport torch\nimport cv2\nfrom tqdm.notebook import tqdm\nfrom pathlib import Path\ncurrent_dir = Path.cwd()\nprint(\"this_dir:\", current_dir)\n\ntarget_dir = Path(\"/kaggle/working/MHAF-YOLO-main\") \nos.chdir(target_dir)\n\nfrom ultralytics import YOLOv10\nimport threading\nimport time\nfrom contextlib import nullcontext\nfrom concurrent.futures import ThreadPoolExecutor\nfrom ultralytics.utils.ops import non_max_suppression\n\n\n# Set random seed for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# Define paths\ndata_path = \"/kaggle/input/byu-locating-bacterial-flagellar-motors-2025/\"\ntest_dir = os.path.join(data_path, \"test\")\nsubmission_path = \"/kaggle/working/submission.csv\"\n\n# Model path - adjust if your best model is saved in a different location\nmodel_path = \"/kaggle/input/mahf-yolo-train/mayolov2f.pt\"\n\n# Detection parameters\nCONFIDENCE_THRESHOLD = 0.7  # Lower threshold to catch more potential motors\nMAX_DETECTIONS_PER_TOMO = 1  # Keep track of top N detections per tomogram\nNMS_IOU_THRESHOLD = 0.2  # Non-maximum suppression threshold for 3D clustering\nCONCENTRATION = 1 # ONLY PROCESS 1/20 slices for fast submission\nSIZE = 1024\n\n# GPU profiling context manager\nclass GPUProfiler:\n    def __init__(self, name):\n        self.name = name\n        self.start_time = None\n        \n    def __enter__(self):\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        self.start_time = time.time()\n        return self\n        \n    def __exit__(self, *args):\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        elapsed = time.time() - self.start_time\n        print(f\"[PROFILE] {self.name}: {elapsed:.3f}s\")\n\n# Check GPU availability and set up optimizations\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n# device = ['cuda:0', 'cuda:1']\nBATCH_SIZE = 8  # Default batch size, will be adjusted dynamically if GPU available\n\nif device.startswith('cuda'):\n    # Set CUDA optimization flags\n    torch.backends.cudnn.benchmark = True\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cuda.matmul.allow_tf32 = True  # Allow TF32 on Ampere GPUs\n    torch.backends.cudnn.allow_tf32 = True\n    \n    # Print GPU info\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9  # Convert to GB\n    print(f\"Using GPU: {gpu_name} with {gpu_mem:.2f} GB memory\")\n    \n    # Get available GPU memory and set batch size accordingly\n    free_mem = gpu_mem - torch.cuda.memory_allocated(0) / 1e9\n    BATCH_SIZE = max(8, min(32, int(free_mem * 4)))  # 4 images per GB as rough estimate\n    print(f\"Dynamic batch size set to {BATCH_SIZE} based on {free_mem:.2f}GB free memory\")\nelse:\n    print(\"GPU not available, using CPU\")\n    BATCH_SIZE = 8  # Reduce batch size for CPU\n\ndef weighted_box_fusion(boxes, iou_threshold=0.4):\n    \"\"\"Applies Weighted Box Fusion to combine overlapping bounding boxes.\"\"\"\n    fused_boxes = []\n    # print(type(boxes))\n    used = [False] * len(boxes)\n\n    for i in range(len(boxes)):\n        if used[i]:\n            continue\n        \n        similar_boxes = [boxes[i]]\n        used[i] = True\n\n        for j in range(i + 1, len(boxes)):\n            if used[j]:\n                continue\n\n            if iou(boxes[i], boxes[j]) > iou_threshold:\n                similar_boxes.append(boxes[j])\n                used[j] = True\n\n        # Compute weighted average for the final box\n        # similar_boxes = np.array(similar_boxes)\n        # similar_boxes = similar_boxes.cpu().numpy()\n        similar_boxes = np.array([t.cpu() for t in similar_boxes])\n        # print(\"similar_boxes: \",similar_boxes)\n        confidences = similar_boxes[:,:, 4]\n        weights = confidences / confidences.sum()\n\n        fused_x1 = np.sum(similar_boxes[:, :, 0] * weights)\n        fused_y1 = np.sum(similar_boxes[:, :, 1] * weights)\n        fused_x2 = np.sum(similar_boxes[:, :, 2] * weights)\n        fused_y2 = np.sum(similar_boxes[:, :, 3] * weights)\n        fused_confidence = np.mean(confidences) #np.max(confidences)  # Take max confidence\n\n        fused_boxes.append((fused_x1, fused_y1, fused_x2, fused_y2, fused_confidence))\n\n    return fused_boxes\n\ndef predict_ensemble_tta(single_model, image_np, device, img_size):\n    \"\"\"\n    For a 640x640 numpy image:\n    - Multiple models (list of models)\n    - Multiple TTA (original, hflip, vflip, rot90)\n    Do the NMS for the last time\n    Return: [K,6] => x1,y1,x2,y2,conf,cls\n    \"\"\"\n    all_boxes = []\n    all_confs = []\n    all_clss = []\n\n    def do_infer(img_tta, invert_func):\n        #for m in models:\n            res = single_model(img_tta, \n                            imgsz=img_size, \n                            # conf=conf_thres,\n                            device=device, \n                            verbose=False)\n            # res = single_model(img_tta,verbose=False)\n            for r in res:\n                # print('res box:', r.boxes)\n                boxes = r.boxes\n                if boxes is None or len(boxes)==0:\n                    # print('predict box is none!')\n                    continue\n                xyxy = boxes.xyxy.cpu().numpy()\n                confs = boxes.conf.cpu().numpy()\n                clss = boxes.cls.cpu().numpy().astype(int)\n                # Inverse transformation\n                xyxy_orig = invert_func(xyxy)\n                all_boxes.append(xyxy_orig)\n                all_confs.append(confs)\n                all_clss.append(clss)\n\n    # Master drawing\n    do_infer(image_np, invert_func=lambda x: x)\n\n    # Horizontal flip\n    img_hflip = cv2.flip(image_np, 1)\n    def invert_hflip(xyxy):\n        new_ = xyxy.copy()\n        x1 = img_size - xyxy[:,2]\n        x2 = img_size - xyxy[:,0]\n        new_[:,0] = x1\n        new_[:,2] = x2\n        return new_\n    do_infer(img_hflip, invert_func=invert_hflip)\n\n    Vertical flip\n    img_vflip = cv2.flip(image_np, 0)\n    def invert_vflip(xyxy):\n        new_ = xyxy.copy()\n        y1 = img_size - xyxy[:,3]\n        y2 = img_size - xyxy[:,1]\n        new_[:,1] = y1\n        new_[:,3] = y2\n        return new_\n    do_infer(img_vflip, invert_func=invert_vflip)\n\n    # Rotate 90 degrees (clockwise)\n    # img_rot90 = cv2.rotate(image_np, cv2.ROTATE_90_CLOCKWISE)\n    # def invert_rot90(xyxy):\n    #     new_ = xyxy.copy()\n    #     # (x,y)->(y,640-x)\n    #     # Inverse transform (x',y')->(640-y',x')\n    #     x1_old,y1_old = xyxy[:,0], xyxy[:,1]\n    #     x2_old,y2_old = xyxy[:,2], xyxy[:,3]\n    #     X1 = img_size - y2_old\n    #     Y1 = x1_old\n    #     X2 = img_size - y1_old\n    #     Y2 = x2_old\n    #     new_[:,0] = X1\n    #     new_[:,1] = Y1\n    #     new_[:,2] = X2\n    #     new_[:,3] = Y2\n    #     return new_\n    # do_infer(img_rot90, invert_func=invert_rot90)\n\n    if len(all_boxes)==0:\n        # print('all boxes is None!')\n        return None\n\n    boxes_cat = np.concatenate(all_boxes, axis=0)\n    confs_cat = np.concatenate(all_confs, axis=0)\n    clss_cat  = np.concatenate(all_clss, axis=0)\n    cat_data = np.column_stack([boxes_cat, confs_cat, clss_cat])  # shape [N,6]\n    # print('final:',cat_data)\n\n    # Need to add batch dimension => [1,N,6]\n    cat_tensor = torch.from_numpy(cat_data).float().unsqueeze(0).to(device)\n    # NMS \n    # nms_out = non_max_suppression(cat_tensor, iou_thres=0.5, max_det=300)\n    nms_out = weighted_box_fusion(cat_tensor, iou_threshold=0.5)\n    # nms_out => list length =1(a graph), take nms_out[0]\n    if len(nms_out)==0 or nms_out[0] is None or len(nms_out[0])==0:\n        return None\n    # final_nms = nms_out[0].cpu().numpy()  # shape [K,6]\n    final_nms = list(nms_out[0]) # shape [K,6]\n    # print(\"final_nms : \", final_nms)\n    return final_nms\n\n\ndef make_predict(sub_path, model, device, img_size):\n    # print(\"img path:\", sub_path)\n    res_list = []\n    for img in sub_path:\n        # print('images:', img)\n        img_np = cv2.imread(img)\n        res_nms = predict_ensemble_tta(model, img_np, device, img_size = img_size)\n        if res_nms is None:\n            # print('nms is none!')\n            continue\n        else:\n            res_list.append(res_nms)\n\n    return res_list\n\n\ndef normalize_slice(slice_data):\n    \"\"\"\n    Normalize slice data using 2nd and 98th percentiles for better contrast\n    \"\"\"\n    p2 = np.percentile(slice_data, 2)\n    p98 = np.percentile(slice_data, 98)\n    clipped_data = np.clip(slice_data, p2, p98)\n    normalized = 255 * (clipped_data - p2) / (p98 - p2)\n    return np.uint8(normalized)\n\ndef preload_image_batch(file_paths):\n    \"\"\"Preload a batch of images to CPU memory\"\"\"\n    images = []\n    for path in file_paths:\n        img = cv2.imread(path)\n        if img is None:\n            # Try with PIL as fallback\n            img = np.array(Image.open(path))\n        images.append(img)\n    return images\n\ndef process_tomogram(tomo_id, model, index=0, total=1,SIZE=SIZE):\n    \"\"\"\n    Process a single tomogram and return the most confident motor detection\n    \"\"\"\n    print(f\"Processing tomogram {tomo_id} ({index}/{total})\")\n    \n    # Get all slice files for this tomogram\n    tomo_dir = os.path.join(test_dir, tomo_id)\n    slice_files = sorted([f for f in os.listdir(tomo_dir) if f.endswith('.jpg')])\n    \n    # Apply CONCENTRATION to reduce the number of slices processed\n    # This will process approximately CONCENTRATION fraction of all slices\n    selected_indices = np.linspace(0, len(slice_files)-1, int(len(slice_files) * CONCENTRATION))\n    selected_indices = np.round(selected_indices).astype(int)\n    slice_files = [slice_files[i] for i in selected_indices]\n    \n    print(f\"Processing {len(slice_files)} out of {len(os.listdir(tomo_dir))} slices based on CONCENTRATION={CONCENTRATION}\")\n    \n    # Create a list to store all detections\n    all_detections = []\n    \n    # Create CUDA streams for parallel processing if using GPU\n    if device.startswith('cuda'):\n        streams = [torch.cuda.Stream() for _ in range(min(8, BATCH_SIZE))]\n    else:\n        streams = [None]\n    \n    # Variables for preloading\n    next_batch_thread = None\n    next_batch_images = None\n    \n    # Process slices in batches\n    for batch_start in range(0, len(slice_files), BATCH_SIZE):\n        # Wait for previous preload thread if it exists\n        if next_batch_thread is not None:\n            next_batch_thread.join()\n            next_batch_images = None\n            \n        batch_end = min(batch_start + BATCH_SIZE, len(slice_files))\n        batch_files = slice_files[batch_start:batch_end]\n        \n        # Start preloading next batch\n        next_batch_start = batch_end\n        next_batch_end = min(next_batch_start + BATCH_SIZE, len(slice_files))\n        next_batch_files = slice_files[next_batch_start:next_batch_end] if next_batch_start < len(slice_files) else []\n        \n        if next_batch_files:\n            next_batch_paths = [os.path.join(tomo_dir, f) for f in next_batch_files]\n            next_batch_thread = threading.Thread(target=preload_image_batch, args=(next_batch_paths,))\n            next_batch_thread.start()\n        else:\n            next_batch_thread = None\n        \n        # Split batch across streams for parallel processing\n        sub_batches = np.array_split(batch_files, len(streams))\n        sub_batch_results = []\n        \n        for i, sub_batch in enumerate(sub_batches):\n            if len(sub_batch) == 0:\n                continue\n                \n            stream = streams[i % len(streams)]\n            with torch.cuda.stream(stream) if stream and device.startswith('cuda') else nullcontext():\n                # Process sub-batch\n                sub_batch_paths = [os.path.join(tomo_dir, slice_file) for slice_file in sub_batch]\n                sub_batch_slice_nums = [int(slice_file.split('_')[1].split('.')[0]) for slice_file in sub_batch]\n                \n                # Run inference with profiling\n                with GPUProfiler(f\"Inference batch {i+1}/{len(sub_batches)}\"):\n                    # sub_results = model(sub_batch_paths, verbose=False)\n                    sub_results = make_predict(sub_batch_paths, model, device, SIZE)\n                    # print(sub_results)\n                \n                # Process each result in this sub-batch\n                # for result in sub_results:\n                    # print('nms_res1:', result)\n                for j,res in enumerate(sub_results):\n                    # print('nms_res2', res)\n                    # x1,y1,x2,y2, confidence, cls_ = res\n                    x1,y1,x2,y2, confidence = res\n                    if confidence >= CONFIDENCE_THRESHOLD:\n                        # Calculate center coordinates\n                        x_center = (x1 + x2) / 2\n                        y_center = (y1 + y2) / 2\n                                \n                        # Store detection with 3D coordinates\n                        all_detections.append({\n                                'z': round(sub_batch_slice_nums[j]),\n                                'y': round(y_center),\n                                'x': round(x_center),\n                                'confidence': float(confidence)\n                            })\n                    # print(\"all_detections:\", all_detections)\n\n                    # if len(result.boxes) > 0:\n                    #     boxes = result.boxes\n                    #     for box_idx, confidence in enumerate(boxes.conf):\n                    #         if confidence >= CONFIDENCE_THRESHOLD:\n                    #             # Get bounding box coordinates\n                    #             x1, y1, x2, y2 = boxes.xyxy[box_idx].cpu().numpy()\n                                \n                    #             # Calculate center coordinates\n                    #             x_center = (x1 + x2) / 2\n                    #             y_center = (y1 + y2) / 2\n                                \n                    #             # Store detection with 3D coordinates\n                    #             all_detections.append({\n                    #                 'z': round(sub_batch_slice_nums[j]),\n                    #                 'y': round(y_center),\n                    #                 'x': round(x_center),\n                    #                 'confidence': float(confidence)\n                    #             })\n        \n        # Synchronize streams\n        if device.startswith('cuda'):\n            torch.cuda.synchronize()\n    \n    # Clean up thread if still running\n    if next_batch_thread is not None:\n        next_batch_thread.join()\n    \n    # 3D Non-Maximum Suppression to merge nearby detections across slices\n    final_detections = perform_3d_nms(all_detections, NMS_IOU_THRESHOLD)\n    \n    # Sort detections by confidence (highest first)\n    final_detections.sort(key=lambda x: x['confidence'], reverse=True)\n    \n    # If there are no detections, return NA values\n    if not final_detections:\n        return {\n            'tomo_id': tomo_id,\n            'Motor axis 0': -1,\n            'Motor axis 1': -1,\n            'Motor axis 2': -1\n        }\n    \n    # Take the detection with highest confidence\n    best_detection = final_detections[0]\n    \n    # Return result with integer coordinates\n    return {\n        'tomo_id': tomo_id,\n        'Motor axis 0': round(best_detection['z']),\n        'Motor axis 1': round(best_detection['y']),\n        'Motor axis 2': round(best_detection['x'])\n    }\n\ndef perform_3d_nms(detections, iou_threshold):\n    \"\"\"\n    Perform 3D Non-Maximum Suppression on detections to merge nearby motors\n    \"\"\"\n    if not detections:\n        return []\n    \n    # Sort by confidence (highest first)\n    detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)\n    \n    # List to store final detections after NMS\n    final_detections = []\n    \n    # Define 3D distance function\n    def distance_3d(d1, d2):\n        return np.sqrt((d1['z'] - d2['z'])**2 + \n                       (d1['y'] - d2['y'])**2 + \n                       (d1['x'] - d2['x'])**2)\n    \n    # Maximum distance threshold (based on box size and slice gap)\n    box_size = 24  # Same as annotation box size\n    distance_threshold = box_size * iou_threshold\n    \n    # Process each detection\n    while detections:\n        # Take the detection with highest confidence\n        best_detection = detections.pop(0)\n        final_detections.append(best_detection)\n        \n        # Filter out detections that are too close to the best detection\n        detections = [d for d in detections if distance_3d(d, best_detection) > distance_threshold]\n    \n    return final_detections\n\ndef generate_submission():\n    \"\"\"\n    Main function to generate the submission file\n    \"\"\"\n    # Get list of test tomograms\n    test_tomos = sorted([d for d in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, d))])\n    total_tomos = len(test_tomos)\n    \n    print(f\"Found {total_tomos} tomograms in test directory\")\n    \n    # Debug image loading for the first tomogram\n    # if test_tomos:\n    #     debug_image_loading(test_tomos[0])\n    \n    # Clear GPU cache before starting\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    # Initialize model once outside the processing loop\n    print(f\"Loading YOLO model from {model_path}\")\n    model = YOLOv10(model_path)\n    model.to(device)\n    \n    # Additional optimizations for inference\n    if device.startswith('cuda'):\n        # Fuse conv and bn layers for faster inference\n        model.fuse()\n        \n        # Enable model half precision (FP16) if on compatible GPU\n        if torch.cuda.get_device_capability(0)[0] >= 7:  # Volta or newer\n            model.model.half()\n            print(\"Using half precision (FP16) for inference\")\n    \n    # Process tomograms with parallelization\n    results = []\n    motors_found = 0\n    \n    # Using ThreadPoolExecutor with max_workers=1 since each worker uses the GPU already\n    # and we're parallelizing within each tomogram processing\n    with ThreadPoolExecutor(max_workers=4) as executor:\n        future_to_tomo = {}\n        \n        # Submit all tomograms for processing\n        for i, tomo_id in enumerate(test_tomos, 1):\n            future = executor.submit(process_tomogram, tomo_id, model, i, total_tomos)\n            future_to_tomo[future] = tomo_id\n        \n        # Process completed futures as they complete\n        for future in future_to_tomo:\n            tomo_id = future_to_tomo[future]\n            if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n                    \n            result = future.result()\n            results.append(result)\n                \n                # Update motors found count\n            has_motor = not pd.isna(result['Motor axis 0'])\n            if has_motor:\n                motors_found += 1\n                print(f\"Motor found in {tomo_id} at position: \"\n                      f\"z={result['Motor axis 0']}, y={result['Motor axis 1']}, x={result['Motor axis 2']}\")\n            else:\n                print(f\"No motor detected in {tomo_id}\")\n                    \n            print(f\"Current detection rate: {motors_found}/{len(results)} ({motors_found/len(results)*100:.1f}%)\")\n            \n            # try:\n            #     # Clear CUDA cache between tomograms\n            #     if torch.cuda.is_available():\n            #         torch.cuda.empty_cache()\n                    \n            #     result = future.result()\n            #     results.append(result)\n                \n            #     # Update motors found count\n            #     has_motor = not pd.isna(result['Motor axis 0'])\n            #     if has_motor:\n            #         motors_found += 1\n            #         print(f\"Motor found in {tomo_id} at position: \"\n            #               f\"z={result['Motor axis 0']}, y={result['Motor axis 1']}, x={result['Motor axis 2']}\")\n            #     else:\n            #         print(f\"No motor detected in {tomo_id}\")\n                    \n            #     print(f\"Current detection rate: {motors_found}/{len(results)} ({motors_found/len(results)*100:.1f}%)\")\n            \n            # except Exception as e:\n            #     print(f\"Error processing {tomo_id}: {e}\")\n            #     # Create a default entry for failed tomograms\n            #     results.append({\n            #         'tomo_id': tomo_id,\n            #         'Motor axis 0': -1,\n            #         'Motor axis 1': -1,\n            #         'Motor axis 2': -1\n            #     })\n    \n    # Create submission dataframe\n    submission_df = pd.DataFrame(results)\n    \n    # Ensure proper column order\n    submission_df = submission_df[['tomo_id', 'Motor axis 0', 'Motor axis 1', 'Motor axis 2']]\n    \n    # Save the submission file\n    submission_df.to_csv(submission_path, index=False)\n    \n    print(f\"\\nSubmission complete!\")\n    print(f\"Motors detected: {motors_found}/{total_tomos} ({motors_found/total_tomos*100:.1f}%)\")\n    print(f\"Submission saved to: {submission_path}\")\n    \n    # Display first few rows of submission\n    print(\"\\nSubmission preview:\")\n    print(submission_df.head())\n    \n    return submission_df","metadata":{"trusted":true,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2025-06-04T00:59:17.357114Z","iopub.execute_input":"2025-06-04T00:59:17.357437Z","iopub.status.idle":"2025-06-04T00:59:29.688532Z","shell.execute_reply.started":"2025-06-04T00:59:17.357412Z","shell.execute_reply":"2025-06-04T00:59:29.687668Z"}},"outputs":[{"name":"stdout","text":"PIP INSTALL OK!!!\nthis_dir: /kaggle/working\nUsing GPU: Tesla P100-PCIE-16GB with 17.06 GB memory\nDynamic batch size set to 32 based on 17.06GB free memory\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Run the submission pipeline\nif __name__ == \"__main__\":\n    # Time entire process\n    start_time = time.time()\n    \n    # Generate submission\n    submission = generate_submission()\n    \n    # Print total execution time\n    elapsed = time.time() - start_time\n    print(f\"\\nTotal execution time: {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T00:59:37.577989Z","iopub.execute_input":"2025-06-04T00:59:37.578450Z","iopub.status.idle":"2025-06-04T01:02:49.160508Z","shell.execute_reply.started":"2025-06-04T00:59:37.578425Z","shell.execute_reply":"2025-06-04T01:02:49.159559Z"}},"outputs":[{"name":"stdout","text":"Found 3 tomograms in test directory\nLoading YOLO model from /kaggle/input/mahf-yolo-train/mayolov2f.pt\n","output_type":"stream"},{"name":"stderr","text":"/kaggle/working/MHAF-YOLO-main/ultralytics/nn/tasks.py:751: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  ckpt = torch.load(file, map_location=\"cpu\")\n","output_type":"stream"},{"name":"stdout","text":"Switch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nMAF-YOLOv10m-v2 summary: 838 layers, 15799254 parameters, 824896 gradients\nProcessing tomogram tomo_003acc (1/3)\nProcessing tomogram tomo_00e047 (2/3)\nProcessing tomogram tomo_01a877 (3/3)\nProcessing 300 out of 300 slices based on CONCENTRATION=1\nProcessing 500 out of 500 slices based on CONCENTRATION=1\nProcessing 300 out of 300 slices based on CONCENTRATION=1\nUltralytics YOLOv8.1.34 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nSwitch model to UniRepLKNetBlock\nMAF-YOLOv10m-v2 summary: 838 layers, 15799254 parameters, 824896 gradients\n[PROFILE] Inference batch 1/8: 26.984s\n[PROFILE] Inference batch 1/8: 27.058s\n[PROFILE] Inference batch 1/8: 27.136s\n[PROFILE] Inference batch 2/8: 1.717s\n[PROFILE] Inference batch 2/8: 1.717s\n[PROFILE] Inference batch 2/8: 1.715s\n[PROFILE] Inference batch 3/8: 1.717s\n[PROFILE] Inference batch 3/8: 1.718s\n[PROFILE] Inference batch 3/8: 1.718s\n[PROFILE] Inference batch 4/8: 1.771s\n[PROFILE] Inference batch 4/8: 1.770s\n[PROFILE] Inference batch 4/8: 1.771s\n[PROFILE] Inference batch 5/8: 1.719s\n[PROFILE] Inference batch 5/8: 1.720s\n[PROFILE] Inference batch 5/8: 1.720s\n[PROFILE] Inference batch 6/8: 1.714s\n[PROFILE] Inference batch 6/8: 1.713s\n[PROFILE] Inference batch 6/8: 1.714s\n[PROFILE] Inference batch 7/8: 1.718s\n[PROFILE] Inference batch 7/8: 1.719s\n[PROFILE] Inference batch 7/8: 1.719s\n[PROFILE] Inference batch 8/8: 1.716s\n[PROFILE] Inference batch 8/8: 1.717s\n[PROFILE] Inference batch 8/8: 1.719s\n[PROFILE] Inference batch 1/8: 1.615s\n[PROFILE] Inference batch 1/8: 1.756s\n[PROFILE] Inference batch 1/8: 1.895s\n[PROFILE] Inference batch 2/8: 1.724s\n[PROFILE] Inference batch 2/8: 1.725s\n[PROFILE] Inference batch 2/8: 1.724s\n[PROFILE] Inference batch 3/8: 1.727s\n[PROFILE] Inference batch 3/8: 1.726s\n[PROFILE] Inference batch 3/8: 1.726s\n[PROFILE] Inference batch 4/8: 1.721s\n[PROFILE] Inference batch 4/8: 1.720s\n[PROFILE] Inference batch 4/8: 1.722s\n[PROFILE] Inference batch 5/8: 1.741s\n[PROFILE] Inference batch 5/8: 1.745s\n[PROFILE] Inference batch 5/8: 1.751s\n[PROFILE] Inference batch 6/8: 1.738s\n[PROFILE] Inference batch 6/8: 1.735s\n[PROFILE] Inference batch 6/8: 1.728s\n[PROFILE] Inference batch 7/8: 1.716s\n[PROFILE] Inference batch 7/8: 1.716s\n[PROFILE] Inference batch 7/8: 1.715s\n[PROFILE] Inference batch 8/8: 1.721s\n[PROFILE] Inference batch 8/8: 1.722s\n[PROFILE] Inference batch 8/8: 1.726s\n[PROFILE] Inference batch 1/8: 1.619s\n[PROFILE] Inference batch 1/8: 1.905s\n[PROFILE] Inference batch 1/8: 1.760s\n[PROFILE] Inference batch 2/8: 1.719s\n[PROFILE] Inference batch 2/8: 1.718s\n[PROFILE] Inference batch 2/8: 1.718s\n[PROFILE] Inference batch 3/8: 1.727s\n[PROFILE] Inference batch 3/8: 1.728s\n[PROFILE] Inference batch 3/8: 1.730s\n[PROFILE] Inference batch 4/8: 1.727s\n[PROFILE] Inference batch 4/8: 1.724s\n[PROFILE] Inference batch 4/8: 1.725s\n[PROFILE] Inference batch 5/8: 1.722s\n[PROFILE] Inference batch 5/8: 1.725s\n[PROFILE] Inference batch 5/8: 1.720s\n[PROFILE] Inference batch 6/8: 1.718s\n[PROFILE] Inference batch 6/8: 1.716s\n[PROFILE] Inference batch 6/8: 1.718s\n[PROFILE] Inference batch 7/8: 1.723s\n[PROFILE] Inference batch 7/8: 1.724s\n[PROFILE] Inference batch 7/8: 1.727s\n[PROFILE] Inference batch 8/8: 1.718s\n[PROFILE] Inference batch 8/8: 1.722s\n[PROFILE] Inference batch 8/8: 1.724s\n[PROFILE] Inference batch 1/8: 1.757s\n[PROFILE] Inference batch 1/8: 1.752s\n[PROFILE] Inference batch 1/8: 1.745s\n[PROFILE] Inference batch 2/8: 1.713s\n[PROFILE] Inference batch 2/8: 1.709s\n[PROFILE] Inference batch 2/8: 1.709s\n[PROFILE] Inference batch 3/8: 1.701s\n[PROFILE] Inference batch 3/8: 1.703s\n[PROFILE] Inference batch 3/8: 1.701s\n[PROFILE] Inference batch 4/8: 1.706s\n[PROFILE] Inference batch 4/8: 1.708s\n[PROFILE] Inference batch 4/8: 1.709s\n[PROFILE] Inference batch 5/8: 1.703s\n[PROFILE] Inference batch 5/8: 1.701s\n[PROFILE] Inference batch 5/8: 1.701s\n[PROFILE] Inference batch 6/8: 1.703s\n[PROFILE] Inference batch 6/8: 1.701s\n[PROFILE] Inference batch 6/8: 1.701s\n[PROFILE] Inference batch 7/8: 1.713s\n[PROFILE] Inference batch 7/8: 1.593s\n[PROFILE] Inference batch 7/8: 1.883s\n[PROFILE] Inference batch 8/8: 1.753s\n[PROFILE] Inference batch 8/8: 1.743s\n[PROFILE] Inference batch 8/8: 1.743s\n[PROFILE] Inference batch 1/8: 1.771s\n[PROFILE] Inference batch 1/8: 1.767s\n[PROFILE] Inference batch 1/8: 1.766s\n[PROFILE] Inference batch 2/8: 1.729s\n[PROFILE] Inference batch 2/8: 1.730s\n[PROFILE] Inference batch 2/8: 1.734s\n[PROFILE] Inference batch 3/8: 1.733s\n[PROFILE] Inference batch 3/8: 1.730s\n[PROFILE] Inference batch 3/8: 1.727s\n[PROFILE] Inference batch 4/8: 1.722s\n[PROFILE] Inference batch 4/8: 1.721s\n[PROFILE] Inference batch 4/8: 1.721s\n[PROFILE] Inference batch 5/8: 1.721s\n[PROFILE] Inference batch 5/8: 1.723s\n[PROFILE] Inference batch 5/8: 1.723s\n[PROFILE] Inference batch 6/8: 1.718s\n[PROFILE] Inference batch 6/8: 1.715s\n[PROFILE] Inference batch 6/8: 1.714s\n[PROFILE] Inference batch 7/8: 1.711s\n[PROFILE] Inference batch 7/8: 1.709s\n[PROFILE] Inference batch 7/8: 1.709s\n[PROFILE] Inference batch 8/8: 1.707s\n[PROFILE] Inference batch 8/8: 1.715s\n[PROFILE] Inference batch 8/8: 1.715s\n[PROFILE] Inference batch 1/8: 1.738s\n[PROFILE] Inference batch 1/8: 1.732s\n[PROFILE] Inference batch 1/8: 1.733s\n[PROFILE] Inference batch 2/8: 1.724s\n[PROFILE] Inference batch 2/8: 1.727s\n[PROFILE] Inference batch 2/8: 1.725s\n[PROFILE] Inference batch 3/8: 1.722s\n[PROFILE] Inference batch 3/8: 1.723s\n[PROFILE] Inference batch 3/8: 1.725s\n[PROFILE] Inference batch 4/8: 1.735s\n[PROFILE] Inference batch 4/8: 1.735s\n[PROFILE] Inference batch 4/8: 1.731s\n[PROFILE] Inference batch 5/8: 1.730s\n[PROFILE] Inference batch 5/8: 1.728s\n[PROFILE] Inference batch 5/8: 1.726s\n[PROFILE] Inference batch 6/8: 1.716s\n[PROFILE] Inference batch 6/8: 1.713s\n[PROFILE] Inference batch 6/8: 1.717s\n[PROFILE] Inference batch 7/8: 1.714s\n[PROFILE] Inference batch 7/8: 1.718s\n[PROFILE] Inference batch 7/8: 1.716s\n[PROFILE] Inference batch 8/8: 1.717s\n[PROFILE] Inference batch 8/8: 1.719s\n[PROFILE] Inference batch 8/8: 1.720s\n[PROFILE] Inference batch 1/8: 1.761s\n[PROFILE] Inference batch 1/8: 1.757s\n[PROFILE] Inference batch 1/8: 1.758s\n[PROFILE] Inference batch 2/8: 1.738s\n[PROFILE] Inference batch 2/8: 1.738s\n[PROFILE] Inference batch 2/8: 1.737s\n[PROFILE] Inference batch 3/8: 1.719s\n[PROFILE] Inference batch 3/8: 1.723s\n[PROFILE] Inference batch 3/8: 1.720s\n[PROFILE] Inference batch 4/8: 1.724s\n[PROFILE] Inference batch 4/8: 1.724s\n[PROFILE] Inference batch 4/8: 1.723s\n[PROFILE] Inference batch 5/8: 1.724s\n[PROFILE] Inference batch 5/8: 1.723s\n[PROFILE] Inference batch 5/8: 1.722s\n[PROFILE] Inference batch 6/8: 1.720s\n[PROFILE] Inference batch 6/8: 1.720s\n[PROFILE] Inference batch 6/8: 1.719s\n[PROFILE] Inference batch 7/8: 1.720s\n[PROFILE] Inference batch 7/8: 1.721s\n[PROFILE] Inference batch 7/8: 1.720s\n[PROFILE] Inference batch 8/8: 1.717s\n[PROFILE] Inference batch 8/8: 1.718s\n[PROFILE] Inference batch 8/8: 1.721s\n[PROFILE] Inference batch 1/8: 1.753s\n[PROFILE] Inference batch 1/8: 1.757s\n[PROFILE] Inference batch 1/8: 1.755s\n[PROFILE] Inference batch 2/8: 1.725s\n[PROFILE] Inference batch 2/8: 1.720s\n[PROFILE] Inference batch 2/8: 1.719s\n[PROFILE] Inference batch 3/8: 1.716s\n[PROFILE] Inference batch 3/8: 1.716s\n[PROFILE] Inference batch 3/8: 1.717s\n[PROFILE] Inference batch 4/8: 1.716s\n[PROFILE] Inference batch 4/8: 1.715s\n[PROFILE] Inference batch 4/8: 1.716s\n[PROFILE] Inference batch 5/8: 1.718s\n[PROFILE] Inference batch 5/8: 1.718s\n[PROFILE] Inference batch 5/8: 1.716s\n[PROFILE] Inference batch 6/8: 1.721s\n[PROFILE] Inference batch 6/8: 1.719s\n[PROFILE] Inference batch 6/8: 1.719s\n[PROFILE] Inference batch 7/8: 1.714s\n[PROFILE] Inference batch 7/8: 1.717s\n[PROFILE] Inference batch 7/8: 1.719s\n[PROFILE] Inference batch 8/8: 1.727s\n[PROFILE] Inference batch 8/8: 1.726s\n[PROFILE] Inference batch 8/8: 1.723s\n[PROFILE] Inference batch 1/8: 1.729s\n[PROFILE] Inference batch 1/8: 1.732s\n[PROFILE] Inference batch 1/8: 1.734s\n[PROFILE] Inference batch 2/8: 1.719s\n[PROFILE] Inference batch 2/8: 1.715s\n[PROFILE] Inference batch 2/8: 1.715s\n[PROFILE] Inference batch 3/8: 1.713s\n[PROFILE] Inference batch 3/8: 1.713s\n[PROFILE] Inference batch 3/8: 1.711s\n[PROFILE] Inference batch 4/8: 1.730s\n[PROFILE] Inference batch 4/8: 1.736s\n[PROFILE] Inference batch 4/8: 1.740s\n[PROFILE] Inference batch 5/8: 1.731s\n[PROFILE] Inference batch 5/8: 1.728s\n[PROFILE] Inference batch 5/8: 1.724s\n[PROFILE] Inference batch 6/8: 1.716s\n[PROFILE] Inference batch 6/8: 1.717s\n[PROFILE] Inference batch 6/8: 1.718s\n[PROFILE] Inference batch 7/8: 1.711s\n[PROFILE] Inference batch 7/8: 1.709s\n[PROFILE] Inference batch 7/8: 1.709s\n[PROFILE] Inference batch 8/8: 1.709s\n[PROFILE] Inference batch 8/8: 1.712s\n[PROFILE] Inference batch 8/8: 1.713s\n[PROFILE] Inference batch 1/8: 0.870s\n[PROFILE] Inference batch 1/8: 0.873s\n[PROFILE] Inference batch 2/8: 0.865s\n[PROFILE] Inference batch 1/8: 1.737s\n[PROFILE] Inference batch 2/8: 0.863s\n[PROFILE] Inference batch 3/8: 0.862s\n[PROFILE] Inference batch 3/8: 0.855s\n[PROFILE] Inference batch 4/8: 0.853s\n[PROFILE] Inference batch 2/8: 1.714s\n[PROFILE] Inference batch 5/8: 0.430s\n[PROFILE] Inference batch 4/8: 0.858s\n[PROFILE] Inference batch 6/8: 0.429s\n[PROFILE] Inference batch 5/8: 0.431s\n[PROFILE] Inference batch 7/8: 0.429s\n[PROFILE] Inference batch 6/8: 0.427s\n[PROFILE] Inference batch 8/8: 0.426s\n[PROFILE] Inference batch 7/8: 0.426s\n[PROFILE] Inference batch 3/8: 1.646s\n[PROFILE] Inference batch 8/8: 0.289s\n[PROFILE] Inference batch 4/8: 0.733s\n[PROFILE] Inference batch 5/8: 0.689s\n[PROFILE] Inference batch 6/8: 0.688s\n[PROFILE] Inference batch 7/8: 0.687s\n[PROFILE] Inference batch 8/8: 0.687s\n[PROFILE] Inference batch 1/8: 0.693s\n[PROFILE] Inference batch 2/8: 0.697s\n[PROFILE] Inference batch 3/8: 0.689s\n[PROFILE] Inference batch 4/8: 0.687s\n[PROFILE] Inference batch 5/8: 0.692s\n[PROFILE] Inference batch 6/8: 0.687s\n[PROFILE] Inference batch 7/8: 0.690s\n[PROFILE] Inference batch 8/8: 0.688s\n[PROFILE] Inference batch 1/8: 0.700s\n[PROFILE] Inference batch 2/8: 0.700s\n[PROFILE] Inference batch 3/8: 0.687s\n[PROFILE] Inference batch 4/8: 0.689s\n[PROFILE] Inference batch 5/8: 0.689s\n[PROFILE] Inference batch 6/8: 0.688s\n[PROFILE] Inference batch 7/8: 0.688s\n[PROFILE] Inference batch 8/8: 0.691s\n[PROFILE] Inference batch 1/8: 0.717s\n[PROFILE] Inference batch 2/8: 0.697s\n[PROFILE] Inference batch 3/8: 0.687s\n[PROFILE] Inference batch 4/8: 0.688s\n[PROFILE] Inference batch 5/8: 0.686s\n[PROFILE] Inference batch 6/8: 0.699s\n[PROFILE] Inference batch 7/8: 0.728s\n[PROFILE] Inference batch 8/8: 0.692s\n[PROFILE] Inference batch 1/8: 0.707s\n[PROFILE] Inference batch 2/8: 0.706s\n[PROFILE] Inference batch 3/8: 0.687s\n[PROFILE] Inference batch 4/8: 0.687s\n[PROFILE] Inference batch 5/8: 0.689s\n[PROFILE] Inference batch 6/8: 0.689s\n[PROFILE] Inference batch 7/8: 0.691s\n[PROFILE] Inference batch 8/8: 0.690s\n[PROFILE] Inference batch 1/8: 0.705s\n[PROFILE] Inference batch 2/8: 0.695s\n[PROFILE] Inference batch 3/8: 0.689s\n[PROFILE] Inference batch 4/8: 0.688s\n[PROFILE] Inference batch 5/8: 0.694s\n[PROFILE] Inference batch 6/8: 0.687s\n[PROFILE] Inference batch 7/8: 0.690s\n[PROFILE] Inference batch 8/8: 0.688s\n[PROFILE] Inference batch 1/8: 0.515s\n[PROFILE] Inference batch 2/8: 0.521s\n[PROFILE] Inference batch 3/8: 0.515s\n[PROFILE] Inference batch 4/8: 0.516s\n[PROFILE] Inference batch 5/8: 0.351s\n[PROFILE] Inference batch 6/8: 0.341s\n[PROFILE] Inference batch 7/8: 0.359s\n[PROFILE] Inference batch 8/8: 0.340s\nMotor found in tomo_003acc at position: z=-1, y=-1, x=-1\nCurrent detection rate: 1/1 (100.0%)\nMotor found in tomo_00e047 at position: z=163, y=546, x=651\nCurrent detection rate: 2/2 (100.0%)\nMotor found in tomo_01a877 at position: z=145, y=638, x=334\nCurrent detection rate: 3/3 (100.0%)\n\nSubmission complete!\nMotors detected: 3/3 (100.0%)\nSubmission saved to: /kaggle/working/submission.csv\n\nSubmission preview:\n       tomo_id  Motor axis 0  Motor axis 1  Motor axis 2\n0  tomo_003acc            -1            -1            -1\n1  tomo_00e047           163           546           651\n2  tomo_01a877           145           638           334\n\nTotal execution time: 191.58 seconds (3.19 minutes)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# tomo_00e047\t169\t546\t603\n# tomo_01a877\t147\t638\t286","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}