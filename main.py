{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7bd96a1",
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2025-06-04T13:20:36.132865Z",
     "iopub.status.busy": "2025-06-04T13:20:36.132600Z",
     "iopub.status.idle": "2025-06-04T13:20:52.655076Z",
     "shell.execute_reply": "2025-06-04T13:20:52.654041Z"
    },
    "papermill": {
     "duration": 16.527989,
     "end_time": "2025-06-04T13:20:52.656695",
     "exception": false,
     "start_time": "2025-06-04T13:20:36.128706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIP INSTALL OK!!!\n",
      "this_dir: /kaggle/working\n",
      "Using GPU: Tesla P100-PCIE-16GB with 17.06 GB memory\n",
      "Dynamic batch size set to 32 based on 17.06GB free memory\n"
     ]
    }
   ],
   "source": [
    "# !tar xfz /kaggle/input/ultralytics-for-offline-install/archive.tar.gz\n",
    "# !pip install --no-index --find-links=./packages -q ultralytics\n",
    "# !rm -rf ./packages\n",
    "# print(\"package installed ...........................\")\n",
    "\n",
    "!cp -r /kaggle/input/mhafyolo/pytorch/default/1/MHAF-YOLO-main /kaggle/working/\n",
    "print('PIP INSTALL OK!!!')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import sys\n",
    "current_dir = Path.cwd()\n",
    "print(\"this_dir:\", current_dir)\n",
    "\n",
    "target_dir = Path(\"/kaggle/working/MHAF-YOLO-main\") \n",
    "os.chdir(target_dir)\n",
    "\n",
    "from ultralytics import YOLOv10\n",
    "import threading\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from ultralytics.utils.ops import non_max_suppression\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define paths\n",
    "data_path = \"/kaggle/input/byu-locating-bacterial-flagellar-motors-2025/\"\n",
    "test_dir = os.path.join(data_path, \"test\")\n",
    "submission_path = \"/kaggle/working/submission.csv\"\n",
    "\n",
    "# Model path - adjust if your best model is saved in a different location\n",
    "model_path = \"/kaggle/input/mahf-yolo-train/mayolov2f.pt\"\n",
    "\n",
    "# Detection parameters\n",
    "CONFIDENCE_THRESHOLD = 0.8  # Lower threshold to catch more potential motors\n",
    "MAX_DETECTIONS_PER_TOMO = 1  # Keep track of top N detections per tomogram\n",
    "NMS_IOU_THRESHOLD = 0.2  # Non-maximum suppression threshold for 3D clustering\n",
    "CONCENTRATION = 1 # ONLY PROCESS 1/20 slices for fast submission\n",
    "SIZE = 1024\n",
    "\n",
    "# GPU profiling context manager\n",
    "class GPUProfiler:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.start_time = None\n",
    "        \n",
    "    def __enter__(self):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        self.start_time = time.time()\n",
    "        return self\n",
    "        \n",
    "    def __exit__(self, *args):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        elapsed = time.time() - self.start_time\n",
    "        print(f\"[PROFILE] {self.name}: {elapsed:.3f}s\")\n",
    "\n",
    "# Check GPU availability and set up optimizations\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "# device = ['cuda:0', 'cuda:1']\n",
    "BATCH_SIZE = 8  # Default batch size, will be adjusted dynamically if GPU available\n",
    "\n",
    "if device.startswith('cuda'):\n",
    "    # Set CUDA optimization flags\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True  # Allow TF32 on Ampere GPUs\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    # Print GPU info\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9  # Convert to GB\n",
    "    print(f\"Using GPU: {gpu_name} with {gpu_mem:.2f} GB memory\")\n",
    "    \n",
    "    # Get available GPU memory and set batch size accordingly\n",
    "    free_mem = gpu_mem - torch.cuda.memory_allocated(0) / 1e9\n",
    "    BATCH_SIZE = max(8, min(32, int(free_mem * 4)))  # 4 images per GB as rough estimate\n",
    "    print(f\"Dynamic batch size set to {BATCH_SIZE} based on {free_mem:.2f}GB free memory\")\n",
    "else:\n",
    "    print(\"GPU not available, using CPU\")\n",
    "    BATCH_SIZE = 8  # Reduce batch size for CPU\n",
    "\n",
    "# Add this IOU function definition\n",
    "def iou(boxA, boxB):\n",
    "    # Determine the coordinates of the intersection rectangle\n",
    "    # boxA and boxB are expected to be (x1, y1, x2, y2, ...)\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    # Compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\n",
    "    # Compute the area of both the prediction and ground-truth rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    unionArea = float(boxAArea + boxBArea - interArea)\n",
    "    if unionArea == 0:\n",
    "        return 0.0 # Avoid division by zero\n",
    "    iou = interArea / unionArea\n",
    "\n",
    "    # Return the intersection over union value\n",
    "    return iou\n",
    "\n",
    "def weighted_box_fusion(boxes, iou_threshold=0.4):\n",
    "    \"\"\"Applies Weighted Box Fusion to combine overlapping bounding boxes.\"\"\"\n",
    "    fused_boxes = []\n",
    "    # print(type(boxes))\n",
    "    used = [False] * len(boxes)\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if used[i]:\n",
    "            continue\n",
    "        \n",
    "        similar_boxes = [boxes[i]]\n",
    "        used[i] = True\n",
    "\n",
    "        for j in range(i + 1, len(boxes)):\n",
    "            if used[j]:\n",
    "                continue\n",
    "\n",
    "            if iou(boxes[i], boxes[j]) > iou_threshold:\n",
    "                similar_boxes.append(boxes[j])\n",
    "                used[j] = True\n",
    "\n",
    "        # Compute weighted average for the final box\n",
    "        # similar_boxes = np.array(similar_boxes)\n",
    "        # similar_boxes = similar_boxes.cpu().numpy()\n",
    "        \n",
    "        # similar_boxes = np.array([t.cpu() for t in similar_boxes])\n",
    "        similar_boxes_np = np.array(similar_boxes)\n",
    "        # print(\"similar_boxes: \",similar_boxes)\n",
    "        confidences = similar_boxes_np[:, 4]\n",
    "        weights = confidences / confidences.sum()\n",
    "\n",
    "        fused_x1 = np.sum(similar_boxes_np[:, 0] * weights)\n",
    "        fused_y1 = np.sum(similar_boxes_np[:, 1] * weights)\n",
    "        fused_x2 = np.sum(similar_boxes_np[:, 2] * weights)\n",
    "        fused_y2 = np.sum(similar_boxes_np[:, 3] * weights)\n",
    "        fused_confidence = np.mean(confidences) # You can keep np.mean or change to np.max\n",
    "\n",
    "        fused_boxes.append((fused_x1, fused_y1, fused_x2, fused_y2, fused_confidence))\n",
    "\n",
    "    return fused_boxes\n",
    "\n",
    "def predict_ensemble_tta(single_model, image_np, device, img_size):\n",
    "    \"\"\"\n",
    "    For a 640x640 numpy image:\n",
    "    - Multiple models (list of models)\n",
    "    - Multiple TTA (original, hflip, vflip, rot90)\n",
    "    Do the NMS for the last time\n",
    "    Return: [K,6] => x1,y1,x2,y2,conf,cls\n",
    "    \"\"\"\n",
    "    all_boxes = []\n",
    "    all_confs = []\n",
    "    all_clss = []\n",
    "\n",
    "    def do_infer(img_tta, invert_func):\n",
    "        #for m in models:\n",
    "            res = single_model(img_tta, \n",
    "                            imgsz=img_size, \n",
    "                            # conf=conf_thres,\n",
    "                            device=device, \n",
    "                            verbose=False)\n",
    "            # res = single_model(img_tta,verbose=False)\n",
    "            for r in res:\n",
    "                # print('res box:', r.boxes)\n",
    "                boxes = r.boxes\n",
    "                if boxes is None or len(boxes)==0:\n",
    "                    # print('predict box is none!')\n",
    "                    continue\n",
    "                xyxy = boxes.xyxy.cpu().numpy()\n",
    "                confs = boxes.conf.cpu().numpy()\n",
    "                clss = boxes.cls.cpu().numpy().astype(int)\n",
    "                # Inverse transformation\n",
    "                xyxy_orig = invert_func(xyxy)\n",
    "                all_boxes.append(xyxy_orig)\n",
    "                all_confs.append(confs)\n",
    "                all_clss.append(clss)\n",
    "\n",
    "    # Master drawing\n",
    "    do_infer(image_np, invert_func=lambda x: x)\n",
    "\n",
    "    # Horizontal flip\n",
    "    img_hflip = cv2.flip(image_np, 1)\n",
    "    def invert_hflip(xyxy):\n",
    "        new_ = xyxy.copy()\n",
    "        x1 = img_size - xyxy[:,2]\n",
    "        x2 = img_size - xyxy[:,0]\n",
    "        new_[:,0] = x1\n",
    "        new_[:,2] = x2\n",
    "        return new_\n",
    "    do_infer(img_hflip, invert_func=invert_hflip)\n",
    "\n",
    "    # Vertical flip\n",
    "    # img_vflip = cv2.flip(image_np, 0)\n",
    "    # def invert_vflip(xyxy):\n",
    "    #     new_ = xyxy.copy()\n",
    "    #     y1 = img_size - xyxy[:,3]\n",
    "    #     y2 = img_size - xyxy[:,1]\n",
    "    #     new_[:,1] = y1\n",
    "    #     new_[:,3] = y2\n",
    "    #     return new_\n",
    "    # do_infer(img_vflip, invert_func=invert_vflip)\n",
    "\n",
    "    # Rotate 90 degrees (clockwise)\n",
    "    # img_rot90 = cv2.rotate(image_np, cv2.ROTATE_90_CLOCKWISE)\n",
    "    # def invert_rot90(xyxy):\n",
    "    #     new_ = xyxy.copy()\n",
    "    #     # (x,y)->(y,640-x)\n",
    "    #     # Inverse transform (x',y')->(640-y',x')\n",
    "    #     x1_old,y1_old = xyxy[:,0], xyxy[:,1]\n",
    "    #     x2_old,y2_old = xyxy[:,2], xyxy[:,3]\n",
    "    #     X1 = img_size - y2_old\n",
    "    #     Y1 = x1_old\n",
    "    #     X2 = img_size - y1_old\n",
    "    #     Y2 = x2_old\n",
    "    #     new_[:,0] = X1\n",
    "    #     new_[:,1] = Y1\n",
    "    #     new_[:,2] = X2\n",
    "    #     new_[:,3] = Y2\n",
    "    #     return new_\n",
    "    # do_infer(img_rot90, invert_func=invert_rot90)\n",
    "\n",
    "    if len(all_boxes)==0:\n",
    "        # print('all boxes is None!')\n",
    "        return None\n",
    "\n",
    "    boxes_cat = np.concatenate(all_boxes, axis=0)\n",
    "    confs_cat = np.concatenate(all_confs, axis=0)\n",
    "    clss_cat  = np.concatenate(all_clss, axis=0)\n",
    "    # Prepare data for weighted_box_fusion. WBF expects a list of individual detections.\n",
    "    # Each detection for WBF should be a tuple/list-like (x1, y1, x2, y2, confidence, [optional class]).\n",
    "    detections_for_wbf = []\n",
    "    for i in range(boxes_cat.shape[0]):\n",
    "        # Append as a list of 6 elements (xyxy, conf, cls) to detections_for_wbf\n",
    "        detections_for_wbf.append([boxes_cat[i,0], boxes_cat[i,1], boxes_cat[i,2], boxes_cat[i,3], confs_cat[i], clss_cat[i]])\n",
    "\n",
    "    # Perform Weighted Box Fusion\n",
    "    # weighted_box_fusion returns a list of (fused_x1, fused_y1, fused_x2, fused_y2, fused_confidence) tuples\n",
    "    fused_boxes = weighted_box_fusion(detections_for_wbf, iou_threshold=0.5)\n",
    "\n",
    "    if not fused_boxes: # If WBF resulted in no boxes\n",
    "        return None\n",
    "\n",
    "    # Select the single best detection after WBF (highest confidence)\n",
    "    best_fused_detection_tuple = max(fused_boxes, key=lambda x: x[4]) # x[4] is confidence\n",
    "\n",
    "    # CRITICAL: Return as a LIST of 5 elements, to be consistent with make_predict's expectation\n",
    "    return list(best_fused_detection_tuple) # Convert the tuple into a list of its elements\n",
    "\n",
    "\n",
    "def make_predict(sub_path, model, device, img_size):\n",
    "    res_list = []\n",
    "    for img_file_path in sub_path:\n",
    "        img_np = cv2.imread(img_file_path)\n",
    "\n",
    "        # Handle cases where image fails to load\n",
    "        if img_np is None:\n",
    "            logger.warning(f\"Could not read image {img_file_path}. Appending zero-confidence placeholder.\")\n",
    "            # Append a LIST of 5 zeros/floats, matching the format of an actual detection list\n",
    "            res_list.append([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "            continue # Move to the next image\n",
    "\n",
    "        # Perform TTA and WBF inference\n",
    "        res_detection_list = predict_ensemble_tta(model, img_np, device, img_size=img_size)\n",
    "\n",
    "        # Handle case where predict_ensemble_tta returns None (no detections found after TTA/WBF)\n",
    "        if res_detection_list is None:\n",
    "            # Append a LIST of 5 zeros/floats as placeholder\n",
    "            res_list.append([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "        else:\n",
    "            # predict_ensemble_tta now returns a LIST of 5 floats: [x1,y1,x2,y2,confidence]\n",
    "            res_list.append(res_detection_list)\n",
    "            \n",
    "    return res_list\n",
    "\n",
    "\n",
    "def normalize_slice(slice_data):\n",
    "    \"\"\"\n",
    "    Normalize slice data using 2nd and 98th percentiles for better contrast\n",
    "    \"\"\"\n",
    "    p2 = np.percentile(slice_data, 2)\n",
    "    p98 = np.percentile(slice_data, 98)\n",
    "    clipped_data = np.clip(slice_data, p2, p98)\n",
    "    normalized = 255 * (clipped_data - p2) / (p98 - p2)\n",
    "    return np.uint8(normalized)\n",
    "\n",
    "def preload_image_batch(file_paths):\n",
    "    \"\"\"Preload a batch of images to CPU memory\"\"\"\n",
    "    images = []\n",
    "    for path in file_paths:\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            # Try with PIL as fallback\n",
    "            img = np.array(Image.open(path))\n",
    "        images.append(img)\n",
    "    return images\n",
    "\n",
    "def process_tomogram(tomo_id, model, index=0, total=1,SIZE=SIZE):\n",
    "    \"\"\"\n",
    "    Process a single tomogram and return the most confident motor detection\n",
    "    \"\"\"\n",
    "    print(f\"Processing tomogram {tomo_id} ({index}/{total})\")\n",
    "    \n",
    "    # Get all slice files for this tomogram\n",
    "    tomo_dir = os.path.join(test_dir, tomo_id)\n",
    "    slice_files = sorted([f for f in os.listdir(tomo_dir) if f.endswith('.jpg')])\n",
    "    \n",
    "    # Apply CONCENTRATION to reduce the number of slices processed\n",
    "    # This will process approximately CONCENTRATION fraction of all slices\n",
    "    selected_indices = np.linspace(0, len(slice_files)-1, int(len(slice_files) * CONCENTRATION))\n",
    "    selected_indices = np.round(selected_indices).astype(int)\n",
    "    slice_files = [slice_files[i] for i in selected_indices]\n",
    "    \n",
    "    print(f\"Processing {len(slice_files)} out of {len(os.listdir(tomo_dir))} slices based on CONCENTRATION={CONCENTRATION}\")\n",
    "    \n",
    "    # Create a list to store all detections\n",
    "    all_detections = []\n",
    "    \n",
    "    # Create CUDA streams for parallel processing if using GPU\n",
    "    if device.startswith('cuda'):\n",
    "        streams = [torch.cuda.Stream() for _ in range(min(8, BATCH_SIZE))]\n",
    "    else:\n",
    "        streams = [None]\n",
    "    \n",
    "    # Variables for preloading\n",
    "    next_batch_thread = None\n",
    "    next_batch_images = None\n",
    "    \n",
    "    # Process slices in batches\n",
    "    for batch_start in range(0, len(slice_files), BATCH_SIZE):\n",
    "        # Wait for previous preload thread if it exists\n",
    "        if next_batch_thread is not None:\n",
    "            next_batch_thread.join()\n",
    "            next_batch_images = None\n",
    "            \n",
    "        batch_end = min(batch_start + BATCH_SIZE, len(slice_files))\n",
    "        batch_files = slice_files[batch_start:batch_end]\n",
    "        \n",
    "        # Start preloading next batch\n",
    "        next_batch_start = batch_end\n",
    "        next_batch_end = min(next_batch_start + BATCH_SIZE, len(slice_files))\n",
    "        next_batch_files = slice_files[next_batch_start:next_batch_end] if next_batch_start < len(slice_files) else []\n",
    "        \n",
    "        if next_batch_files:\n",
    "            next_batch_paths = [os.path.join(tomo_dir, f) for f in next_batch_files]\n",
    "            next_batch_thread = threading.Thread(target=preload_image_batch, args=(next_batch_paths,))\n",
    "            next_batch_thread.start()\n",
    "        else:\n",
    "            next_batch_thread = None\n",
    "        \n",
    "        # Split batch across streams for parallel processing\n",
    "        sub_batches = np.array_split(batch_files, len(streams))\n",
    "        sub_batch_results = []\n",
    "        \n",
    "        for i, sub_batch in enumerate(sub_batches):\n",
    "            if len(sub_batch) == 0:\n",
    "                continue\n",
    "                \n",
    "            stream = streams[i % len(streams)]\n",
    "            with torch.cuda.stream(stream) if stream and device.startswith('cuda') else nullcontext():\n",
    "                # Process sub-batch\n",
    "                sub_batch_paths = [os.path.join(tomo_dir, slice_file) for slice_file in sub_batch]\n",
    "                sub_batch_slice_nums = [int(slice_file.split('_')[1].split('.')[0]) for slice_file in sub_batch]\n",
    "                \n",
    "                # Run inference with profiling\n",
    "                with GPUProfiler(f\"Inference batch {i+1}/{len(sub_batches)}\"):\n",
    "                    # sub_results = model(sub_batch_paths, verbose=False)\n",
    "                    sub_results = make_predict(sub_batch_paths, model, device, SIZE)\n",
    "                    # print(sub_results)\n",
    "                \n",
    "                # Process each result in this sub-batch\n",
    "                # for result in sub_results:\n",
    "                    # print('nms_res1:', result)\n",
    "                for j,res in enumerate(sub_results):\n",
    "                    # print('nms_res2', res)\n",
    "                    # x1,y1,x2,y2, confidence, cls_ = res\n",
    "                    x1,y1,x2,y2, confidence = res\n",
    "                    if confidence >= CONFIDENCE_THRESHOLD:\n",
    "                        # Calculate center coordinates\n",
    "                        x_center = (x1 + x2) / 2\n",
    "                        y_center = (y1 + y2) / 2\n",
    "                                \n",
    "                        # Store detection with 3D coordinates\n",
    "                        all_detections.append({\n",
    "                                'z': round(sub_batch_slice_nums[j]),\n",
    "                                'y': round(y_center),\n",
    "                                'x': round(x_center),\n",
    "                                'confidence': float(confidence)\n",
    "                            })\n",
    "        \n",
    "        # Synchronize streams\n",
    "        if device.startswith('cuda'):\n",
    "            torch.cuda.synchronize()\n",
    "    \n",
    "    # Clean up thread if still running\n",
    "    if next_batch_thread is not None:\n",
    "        next_batch_thread.join()\n",
    "    \n",
    "    # 3D Non-Maximum Suppression to merge nearby detections across slices\n",
    "    final_detections = perform_3d_nms(all_detections, NMS_IOU_THRESHOLD)\n",
    "    \n",
    "    # Sort detections by confidence (highest first)\n",
    "    final_detections.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    # If there are no detections, return NA values\n",
    "    if not final_detections:\n",
    "        return {\n",
    "            'tomo_id': tomo_id,\n",
    "            'Motor axis 0': -1,\n",
    "            'Motor axis 1': -1,\n",
    "            'Motor axis 2': -1\n",
    "        }\n",
    "    \n",
    "    # Take the detection with highest confidence\n",
    "    best_detection = final_detections[0]\n",
    "    \n",
    "    # Return result with integer coordinates\n",
    "    return {\n",
    "        'tomo_id': tomo_id,\n",
    "        'Motor axis 0': round(best_detection['z']),\n",
    "        'Motor axis 1': round(best_detection['y']),\n",
    "        'Motor axis 2': round(best_detection['x'])\n",
    "    }\n",
    "\n",
    "def perform_3d_nms(detections, iou_threshold):\n",
    "    \"\"\"\n",
    "    Perform 3D Non-Maximum Suppression on detections to merge nearby motors\n",
    "    \"\"\"\n",
    "    if not detections:\n",
    "        return []\n",
    "    \n",
    "    # Sort by confidence (highest first)\n",
    "    detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    # List to store final detections after NMS\n",
    "    final_detections = []\n",
    "    \n",
    "    # Define 3D distance function\n",
    "    def distance_3d(d1, d2):\n",
    "        return np.sqrt((d1['z'] - d2['z'])**2 + \n",
    "                       (d1['y'] - d2['y'])**2 + \n",
    "                       (d1['x'] - d2['x'])**2)\n",
    "    \n",
    "    # Maximum distance threshold (based on box size and slice gap)\n",
    "    box_size = 24  # Same as annotation box size\n",
    "    distance_threshold = box_size * iou_threshold\n",
    "    \n",
    "    # Process each detection\n",
    "    while detections:\n",
    "        # Take the detection with highest confidence\n",
    "        best_detection = detections.pop(0)\n",
    "        final_detections.append(best_detection)\n",
    "        \n",
    "        # Filter out detections that are too close to the best detection\n",
    "        detections = [d for d in detections if distance_3d(d, best_detection) > distance_threshold]\n",
    "    \n",
    "    return final_detections\n",
    "\n",
    "def generate_submission():\n",
    "    \"\"\"\n",
    "    Main function to generate the submission file\n",
    "    \"\"\"\n",
    "    # Get list of test tomograms\n",
    "    test_tomos = sorted([d for d in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, d))])\n",
    "    total_tomos = len(test_tomos)\n",
    "    \n",
    "    print(f\"Found {total_tomos} tomograms in test directory\")\n",
    "    \n",
    "    # Debug image loading for the first tomogram\n",
    "    # if test_tomos:\n",
    "    #     debug_image_loading(test_tomos[0])\n",
    "    \n",
    "    # Clear GPU cache before starting\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Initialize model once outside the processing loop\n",
    "    print(f\"Loading YOLO model from {model_path}\")\n",
    "    model = YOLOv10(model_path)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Additional optimizations for inference\n",
    "    if device.startswith('cuda'):\n",
    "        # Fuse conv and bn layers for faster inference\n",
    "        model.fuse()\n",
    "        \n",
    "        # Enable model half precision (FP16) if on compatible GPU\n",
    "        if torch.cuda.get_device_capability(0)[0] >= 7:  # Volta or newer\n",
    "            model.model.half()\n",
    "            print(\"Using half precision (FP16) for inference\")\n",
    "    \n",
    "    # Process tomograms with parallelization\n",
    "    results = []\n",
    "    motors_found = 0\n",
    "    \n",
    "    # Using ThreadPoolExecutor with max_workers=1 since each worker uses the GPU already\n",
    "    # and we're parallelizing within each tomogram processing\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        future_to_tomo = {}\n",
    "        \n",
    "        # Submit all tomograms for processing\n",
    "        for i, tomo_id in enumerate(test_tomos, 1):\n",
    "            future = executor.submit(process_tomogram, tomo_id, model, i, total_tomos)\n",
    "            future_to_tomo[future] = tomo_id\n",
    "        \n",
    "        # Process completed futures as they complete\n",
    "        for future in future_to_tomo:\n",
    "            tomo_id = future_to_tomo[future]\n",
    "            if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "                \n",
    "                # Update motors found count\n",
    "            has_motor = not pd.isna(result['Motor axis 0'])\n",
    "            if has_motor:\n",
    "                motors_found += 1\n",
    "                print(f\"Motor found in {tomo_id} at position: \"\n",
    "                      f\"z={result['Motor axis 0']}, y={result['Motor axis 1']}, x={result['Motor axis 2']}\")\n",
    "            else:\n",
    "                print(f\"No motor detected in {tomo_id}\")\n",
    "                    \n",
    "            print(f\"Current detection rate: {motors_found}/{len(results)} ({motors_found/len(results)*100:.1f}%)\")\n",
    "            \n",
    "    # Create submission dataframe\n",
    "    submission_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Ensure proper column order\n",
    "    submission_df = submission_df[['tomo_id', 'Motor axis 0', 'Motor axis 1', 'Motor axis 2']]\n",
    "    \n",
    "    # Save the submission file\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "    print(f\"\\nSubmission complete!\")\n",
    "    print(f\"Motors detected: {motors_found}/{total_tomos} ({motors_found/total_tomos*100:.1f}%)\")\n",
    "    print(f\"Submission saved to: {submission_path}\")\n",
    "    \n",
    "    # Display first few rows of submission\n",
    "    print(\"\\nSubmission preview:\")\n",
    "    print(submission_df.head())\n",
    "    \n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24e8fd96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T13:20:52.662233Z",
     "iopub.status.busy": "2025-06-04T13:20:52.661823Z",
     "iopub.status.idle": "2025-06-04T13:24:02.452356Z",
     "shell.execute_reply": "2025-06-04T13:24:02.451259Z"
    },
    "papermill": {
     "duration": 189.794647,
     "end_time": "2025-06-04T13:24:02.453787",
     "exception": false,
     "start_time": "2025-06-04T13:20:52.659140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 tomograms in test directory\n",
      "Loading YOLO model from /kaggle/input/mahf-yolo-train/mayolov2f.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/working/MHAF-YOLO-main/ultralytics/nn/tasks.py:751: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "MAF-YOLOv10m-v2 summary: 838 layers, 15799254 parameters, 824896 gradients\n",
      "Processing tomogram tomo_003acc (1/3)\n",
      "Processing tomogram tomo_00e047 (2/3)\n",
      "Processing tomogram tomo_01a877 (3/3)\n",
      "Processing 500 out of 500 slices based on CONCENTRATION=1\n",
      "Processing 300 out of 300 slices based on CONCENTRATION=1\n",
      "Processing 300 out of 300 slices based on CONCENTRATION=1\n",
      "Ultralytics YOLOv8.1.34 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Ultralytics YOLOv8.1.34 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "Switch model to UniRepLKNetBlock\n",
      "MAF-YOLOv10m-v2 summary: 838 layers, 15799254 parameters, 824896 gradients\n",
      "MAF-YOLOv10m-v2 summary: 838 layers, 15799254 parameters, 824896 gradients\n",
      "[PROFILE] Inference batch 1/8: 26.652s\n",
      "[PROFILE] Inference batch 1/8: 26.736s\n",
      "[PROFILE] Inference batch 1/8: 26.807s\n",
      "[PROFILE] Inference batch 2/8: 1.705s\n",
      "[PROFILE] Inference batch 2/8: 1.704s\n",
      "[PROFILE] Inference batch 2/8: 1.704s\n",
      "[PROFILE] Inference batch 3/8: 1.705s\n",
      "[PROFILE] Inference batch 3/8: 1.706s\n",
      "[PROFILE] Inference batch 3/8: 1.706s\n",
      "[PROFILE] Inference batch 4/8: 1.763s\n",
      "[PROFILE] Inference batch 4/8: 1.762s\n",
      "[PROFILE] Inference batch 4/8: 1.762s\n",
      "[PROFILE] Inference batch 5/8: 1.698s\n",
      "[PROFILE] Inference batch 5/8: 1.698s\n",
      "[PROFILE] Inference batch 5/8: 1.698s\n",
      "[PROFILE] Inference batch 6/8: 1.701s\n",
      "[PROFILE] Inference batch 6/8: 1.703s\n",
      "[PROFILE] Inference batch 6/8: 1.703s\n",
      "[PROFILE] Inference batch 7/8: 1.702s\n",
      "[PROFILE] Inference batch 7/8: 1.702s\n",
      "[PROFILE] Inference batch 7/8: 1.702s\n",
      "[PROFILE] Inference batch 8/8: 1.697s\n",
      "[PROFILE] Inference batch 8/8: 1.700s\n",
      "[PROFILE] Inference batch 8/8: 1.701s\n",
      "[PROFILE] Inference batch 1/8: 1.593s\n",
      "[PROFILE] Inference batch 1/8: 1.720s\n",
      "[PROFILE] Inference batch 1/8: 1.870s\n",
      "[PROFILE] Inference batch 2/8: 1.700s\n",
      "[PROFILE] Inference batch 2/8: 1.700s\n",
      "[PROFILE] Inference batch 2/8: 1.701s\n",
      "[PROFILE] Inference batch 3/8: 1.701s\n",
      "[PROFILE] Inference batch 3/8: 1.702s\n",
      "[PROFILE] Inference batch 3/8: 1.702s\n",
      "[PROFILE] Inference batch 4/8: 1.704s\n",
      "[PROFILE] Inference batch 4/8: 1.704s\n",
      "[PROFILE] Inference batch 4/8: 1.704s\n",
      "[PROFILE] Inference batch 5/8: 1.707s\n",
      "[PROFILE] Inference batch 5/8: 1.707s\n",
      "[PROFILE] Inference batch 5/8: 1.710s\n",
      "[PROFILE] Inference batch 6/8: 1.729s\n",
      "[PROFILE] Inference batch 6/8: 1.732s\n",
      "[PROFILE] Inference batch 6/8: 1.732s\n",
      "[PROFILE] Inference batch 7/8: 1.712s\n",
      "[PROFILE] Inference batch 7/8: 1.710s\n",
      "[PROFILE] Inference batch 7/8: 1.707s\n",
      "[PROFILE] Inference batch 8/8: 1.701s\n",
      "[PROFILE] Inference batch 8/8: 1.704s\n",
      "[PROFILE] Inference batch 8/8: 1.708s\n",
      "[PROFILE] Inference batch 1/8: 1.738s\n",
      "[PROFILE] Inference batch 1/8: 1.734s\n",
      "[PROFILE] Inference batch 1/8: 1.733s\n",
      "[PROFILE] Inference batch 2/8: 1.709s\n",
      "[PROFILE] Inference batch 2/8: 1.708s\n",
      "[PROFILE] Inference batch 2/8: 1.705s\n",
      "[PROFILE] Inference batch 3/8: 1.701s\n",
      "[PROFILE] Inference batch 3/8: 1.703s\n",
      "[PROFILE] Inference batch 3/8: 1.703s\n",
      "[PROFILE] Inference batch 4/8: 1.703s\n",
      "[PROFILE] Inference batch 4/8: 1.702s\n",
      "[PROFILE] Inference batch 4/8: 1.702s\n",
      "[PROFILE] Inference batch 5/8: 1.702s\n",
      "[PROFILE] Inference batch 5/8: 1.701s\n",
      "[PROFILE] Inference batch 5/8: 1.701s\n",
      "[PROFILE] Inference batch 6/8: 1.704s\n",
      "[PROFILE] Inference batch 6/8: 1.702s\n",
      "[PROFILE] Inference batch 6/8: 1.703s\n",
      "[PROFILE] Inference batch 7/8: 1.702s\n",
      "[PROFILE] Inference batch 7/8: 1.703s\n",
      "[PROFILE] Inference batch 7/8: 1.702s\n",
      "[PROFILE] Inference batch 8/8: 1.706s\n",
      "[PROFILE] Inference batch 8/8: 1.711s\n",
      "[PROFILE] Inference batch 8/8: 1.716s\n",
      "[PROFILE] Inference batch 1/8: 1.750s\n",
      "[PROFILE] Inference batch 1/8: 1.743s\n",
      "[PROFILE] Inference batch 1/8: 1.737s\n",
      "[PROFILE] Inference batch 2/8: 1.703s\n",
      "[PROFILE] Inference batch 2/8: 1.705s\n",
      "[PROFILE] Inference batch 2/8: 1.704s\n",
      "[PROFILE] Inference batch 3/8: 1.704s\n",
      "[PROFILE] Inference batch 3/8: 1.705s\n",
      "[PROFILE] Inference batch 3/8: 1.704s\n",
      "[PROFILE] Inference batch 4/8: 1.701s\n",
      "[PROFILE] Inference batch 4/8: 1.701s\n",
      "[PROFILE] Inference batch 4/8: 1.702s\n",
      "[PROFILE] Inference batch 5/8: 1.705s\n",
      "[PROFILE] Inference batch 5/8: 1.702s\n",
      "[PROFILE] Inference batch 5/8: 1.705s\n",
      "[PROFILE] Inference batch 6/8: 1.706s\n",
      "[PROFILE] Inference batch 6/8: 1.708s\n",
      "[PROFILE] Inference batch 6/8: 1.704s\n",
      "[PROFILE] Inference batch 7/8: 1.698s\n",
      "[PROFILE] Inference batch 7/8: 1.697s\n",
      "[PROFILE] Inference batch 7/8: 1.697s\n",
      "[PROFILE] Inference batch 8/8: 1.707s\n",
      "[PROFILE] Inference batch 8/8: 1.718s\n",
      "[PROFILE] Inference batch 8/8: 1.724s\n",
      "[PROFILE] Inference batch 1/8: 1.768s\n",
      "[PROFILE] Inference batch 1/8: 1.621s\n",
      "[PROFILE] Inference batch 1/8: 1.896s\n",
      "[PROFILE] Inference batch 2/8: 1.703s\n",
      "[PROFILE] Inference batch 2/8: 1.703s\n",
      "[PROFILE] Inference batch 2/8: 1.704s\n",
      "[PROFILE] Inference batch 3/8: 1.708s\n",
      "[PROFILE] Inference batch 3/8: 1.706s\n",
      "[PROFILE] Inference batch 3/8: 1.710s\n",
      "[PROFILE] Inference batch 4/8: 1.708s\n",
      "[PROFILE] Inference batch 4/8: 1.706s\n",
      "[PROFILE] Inference batch 4/8: 1.703s\n",
      "[PROFILE] Inference batch 5/8: 1.707s\n",
      "[PROFILE] Inference batch 5/8: 1.710s\n",
      "[PROFILE] Inference batch 5/8: 1.711s\n",
      "[PROFILE] Inference batch 6/8: 1.706s\n",
      "[PROFILE] Inference batch 6/8: 1.705s\n",
      "[PROFILE] Inference batch 6/8: 1.707s\n",
      "[PROFILE] Inference batch 7/8: 1.707s\n",
      "[PROFILE] Inference batch 7/8: 1.708s\n",
      "[PROFILE] Inference batch 7/8: 1.704s\n",
      "[PROFILE] Inference batch 8/8: 1.705s\n",
      "[PROFILE] Inference batch 8/8: 1.706s\n",
      "[PROFILE] Inference batch 8/8: 1.713s\n",
      "[PROFILE] Inference batch 1/8: 1.729s\n",
      "[PROFILE] Inference batch 1/8: 1.732s\n",
      "[PROFILE] Inference batch 1/8: 1.728s\n",
      "[PROFILE] Inference batch 2/8: 1.710s\n",
      "[PROFILE] Inference batch 2/8: 1.705s\n",
      "[PROFILE] Inference batch 2/8: 1.701s\n",
      "[PROFILE] Inference batch 3/8: 1.699s\n",
      "[PROFILE] Inference batch 3/8: 1.699s\n",
      "[PROFILE] Inference batch 3/8: 1.699s\n",
      "[PROFILE] Inference batch 4/8: 1.699s\n",
      "[PROFILE] Inference batch 4/8: 1.701s\n",
      "[PROFILE] Inference batch 4/8: 1.701s\n",
      "[PROFILE] Inference batch 5/8: 1.702s\n",
      "[PROFILE] Inference batch 5/8: 1.701s\n",
      "[PROFILE] Inference batch 5/8: 1.702s\n",
      "[PROFILE] Inference batch 6/8: 1.702s\n",
      "[PROFILE] Inference batch 6/8: 1.703s\n",
      "[PROFILE] Inference batch 6/8: 1.702s\n",
      "[PROFILE] Inference batch 7/8: 1.706s\n",
      "[PROFILE] Inference batch 7/8: 1.705s\n",
      "[PROFILE] Inference batch 7/8: 1.706s\n",
      "[PROFILE] Inference batch 8/8: 1.702s\n",
      "[PROFILE] Inference batch 8/8: 1.704s\n",
      "[PROFILE] Inference batch 8/8: 1.716s\n",
      "[PROFILE] Inference batch 1/8: 1.736s\n",
      "[PROFILE] Inference batch 1/8: 1.733s\n",
      "[PROFILE] Inference batch 1/8: 1.720s\n",
      "[PROFILE] Inference batch 2/8: 1.699s\n",
      "[PROFILE] Inference batch 2/8: 1.704s\n",
      "[PROFILE] Inference batch 2/8: 1.705s\n",
      "[PROFILE] Inference batch 3/8: 1.728s\n",
      "[PROFILE] Inference batch 3/8: 1.730s\n",
      "[PROFILE] Inference batch 3/8: 1.737s\n",
      "[PROFILE] Inference batch 4/8: 1.723s\n",
      "[PROFILE] Inference batch 4/8: 1.720s\n",
      "[PROFILE] Inference batch 4/8: 1.716s\n",
      "[PROFILE] Inference batch 5/8: 1.711s\n",
      "[PROFILE] Inference batch 5/8: 1.708s\n",
      "[PROFILE] Inference batch 5/8: 1.705s\n",
      "[PROFILE] Inference batch 6/8: 1.700s\n",
      "[PROFILE] Inference batch 6/8: 1.701s\n",
      "[PROFILE] Inference batch 6/8: 1.701s\n",
      "[PROFILE] Inference batch 7/8: 1.705s\n",
      "[PROFILE] Inference batch 7/8: 1.707s\n",
      "[PROFILE] Inference batch 7/8: 1.707s\n",
      "[PROFILE] Inference batch 8/8: 1.705s\n",
      "[PROFILE] Inference batch 8/8: 1.707s\n",
      "[PROFILE] Inference batch 8/8: 1.719s\n",
      "[PROFILE] Inference batch 1/8: 1.748s\n",
      "[PROFILE] Inference batch 1/8: 1.746s\n",
      "[PROFILE] Inference batch 1/8: 1.735s\n",
      "[PROFILE] Inference batch 2/8: 1.700s\n",
      "[PROFILE] Inference batch 2/8: 1.701s\n",
      "[PROFILE] Inference batch 2/8: 1.700s\n",
      "[PROFILE] Inference batch 3/8: 1.706s\n",
      "[PROFILE] Inference batch 3/8: 1.703s\n",
      "[PROFILE] Inference batch 3/8: 1.702s\n",
      "[PROFILE] Inference batch 4/8: 1.699s\n",
      "[PROFILE] Inference batch 4/8: 1.700s\n",
      "[PROFILE] Inference batch 4/8: 1.700s\n",
      "[PROFILE] Inference batch 5/8: 1.699s\n",
      "[PROFILE] Inference batch 5/8: 1.703s\n",
      "[PROFILE] Inference batch 5/8: 1.704s\n",
      "[PROFILE] Inference batch 6/8: 1.704s\n",
      "[PROFILE] Inference batch 6/8: 1.702s\n",
      "[PROFILE] Inference batch 6/8: 1.702s\n",
      "[PROFILE] Inference batch 7/8: 1.699s\n",
      "[PROFILE] Inference batch 7/8: 1.702s\n",
      "[PROFILE] Inference batch 7/8: 1.700s\n",
      "[PROFILE] Inference batch 8/8: 1.701s\n",
      "[PROFILE] Inference batch 8/8: 1.701s\n",
      "[PROFILE] Inference batch 8/8: 1.700s\n",
      "[PROFILE] Inference batch 1/8: 1.753s\n",
      "[PROFILE] Inference batch 1/8: 1.752s\n",
      "[PROFILE] Inference batch 1/8: 1.752s\n",
      "[PROFILE] Inference batch 2/8: 1.700s\n",
      "[PROFILE] Inference batch 2/8: 1.698s\n",
      "[PROFILE] Inference batch 2/8: 1.697s\n",
      "[PROFILE] Inference batch 3/8: 1.694s\n",
      "[PROFILE] Inference batch 3/8: 1.696s\n",
      "[PROFILE] Inference batch 3/8: 1.695s\n",
      "[PROFILE] Inference batch 4/8: 1.697s\n",
      "[PROFILE] Inference batch 4/8: 1.697s\n",
      "[PROFILE] Inference batch 4/8: 1.698s\n",
      "[PROFILE] Inference batch 5/8: 1.702s\n",
      "[PROFILE] Inference batch 5/8: 1.707s\n",
      "[PROFILE] Inference batch 5/8: 1.715s\n",
      "[PROFILE] Inference batch 6/8: 1.717s\n",
      "[PROFILE] Inference batch 6/8: 1.712s\n",
      "[PROFILE] Inference batch 6/8: 1.703s\n",
      "[PROFILE] Inference batch 7/8: 1.700s\n",
      "[PROFILE] Inference batch 7/8: 1.699s\n",
      "[PROFILE] Inference batch 7/8: 1.699s\n",
      "[PROFILE] Inference batch 8/8: 1.692s\n",
      "[PROFILE] Inference batch 8/8: 1.692s\n",
      "[PROFILE] Inference batch 8/8: 1.692s\n",
      "[PROFILE] Inference batch 1/8: 0.858s\n",
      "[PROFILE] Inference batch 1/8: 0.862s\n",
      "[PROFILE] Inference batch 2/8: 0.856s\n",
      "[PROFILE] Inference batch 2/8: 0.852s\n",
      "[PROFILE] Inference batch 1/8: 1.714s\n",
      "[PROFILE] Inference batch 3/8: 0.850s\n",
      "[PROFILE] Inference batch 3/8: 0.851s\n",
      "[PROFILE] Inference batch 4/8: 0.844s\n",
      "[PROFILE] Inference batch 4/8: 0.846s\n",
      "[PROFILE] Inference batch 5/8: 0.423s\n",
      "[PROFILE] Inference batch 2/8: 1.700s\n",
      "[PROFILE] Inference batch 5/8: 0.432s\n",
      "[PROFILE] Inference batch 6/8: 0.432s\n",
      "[PROFILE] Inference batch 6/8: 0.424s\n",
      "[PROFILE] Inference batch 7/8: 0.421s\n",
      "[PROFILE] Inference batch 7/8: 0.422s\n",
      "[PROFILE] Inference batch 8/8: 0.422s\n",
      "[PROFILE] Inference batch 8/8: 0.421s\n",
      "[PROFILE] Inference batch 3/8: 1.489s\n",
      "[PROFILE] Inference batch 4/8: 0.683s\n",
      "[PROFILE] Inference batch 5/8: 0.703s\n",
      "[PROFILE] Inference batch 6/8: 0.691s\n",
      "[PROFILE] Inference batch 7/8: 0.684s\n",
      "[PROFILE] Inference batch 8/8: 0.676s\n",
      "[PROFILE] Inference batch 1/8: 0.691s\n",
      "[PROFILE] Inference batch 2/8: 0.703s\n",
      "[PROFILE] Inference batch 3/8: 0.677s\n",
      "[PROFILE] Inference batch 4/8: 0.677s\n",
      "[PROFILE] Inference batch 5/8: 0.677s\n",
      "[PROFILE] Inference batch 6/8: 0.683s\n",
      "[PROFILE] Inference batch 7/8: 0.681s\n",
      "[PROFILE] Inference batch 8/8: 0.679s\n",
      "[PROFILE] Inference batch 1/8: 0.697s\n",
      "[PROFILE] Inference batch 2/8: 0.686s\n",
      "[PROFILE] Inference batch 3/8: 0.678s\n",
      "[PROFILE] Inference batch 4/8: 0.685s\n",
      "[PROFILE] Inference batch 5/8: 0.684s\n",
      "[PROFILE] Inference batch 6/8: 0.683s\n",
      "[PROFILE] Inference batch 7/8: 0.679s\n",
      "[PROFILE] Inference batch 8/8: 0.677s\n",
      "[PROFILE] Inference batch 1/8: 0.707s\n",
      "[PROFILE] Inference batch 2/8: 0.692s\n",
      "[PROFILE] Inference batch 3/8: 0.679s\n",
      "[PROFILE] Inference batch 4/8: 0.678s\n",
      "[PROFILE] Inference batch 5/8: 0.680s\n",
      "[PROFILE] Inference batch 6/8: 0.678s\n",
      "[PROFILE] Inference batch 7/8: 0.684s\n",
      "[PROFILE] Inference batch 8/8: 0.679s\n",
      "[PROFILE] Inference batch 1/8: 0.689s\n",
      "[PROFILE] Inference batch 2/8: 0.730s\n",
      "[PROFILE] Inference batch 3/8: 0.702s\n",
      "[PROFILE] Inference batch 4/8: 0.680s\n",
      "[PROFILE] Inference batch 5/8: 0.680s\n",
      "[PROFILE] Inference batch 6/8: 0.678s\n",
      "[PROFILE] Inference batch 7/8: 0.678s\n",
      "[PROFILE] Inference batch 8/8: 0.692s\n",
      "[PROFILE] Inference batch 1/8: 0.694s\n",
      "[PROFILE] Inference batch 2/8: 0.681s\n",
      "[PROFILE] Inference batch 3/8: 0.680s\n",
      "[PROFILE] Inference batch 4/8: 0.680s\n",
      "[PROFILE] Inference batch 5/8: 0.682s\n",
      "[PROFILE] Inference batch 6/8: 0.679s\n",
      "[PROFILE] Inference batch 7/8: 0.677s\n",
      "[PROFILE] Inference batch 8/8: 0.679s\n",
      "[PROFILE] Inference batch 1/8: 0.507s\n",
      "[PROFILE] Inference batch 2/8: 0.525s\n",
      "[PROFILE] Inference batch 3/8: 0.507s\n",
      "[PROFILE] Inference batch 4/8: 0.511s\n",
      "[PROFILE] Inference batch 5/8: 0.347s\n",
      "[PROFILE] Inference batch 6/8: 0.344s\n",
      "[PROFILE] Inference batch 7/8: 0.351s\n",
      "[PROFILE] Inference batch 8/8: 0.336s\n",
      "Motor found in tomo_003acc at position: z=179, y=245, x=761\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Motor found in tomo_00e047 at position: z=163, y=546, x=699\n",
      "Current detection rate: 2/2 (100.0%)\n",
      "Motor found in tomo_01a877 at position: z=147, y=638, x=286\n",
      "Current detection rate: 3/3 (100.0%)\n",
      "\n",
      "Submission complete!\n",
      "Motors detected: 3/3 (100.0%)\n",
      "Submission saved to: /kaggle/working/submission.csv\n",
      "\n",
      "Submission preview:\n",
      "       tomo_id  Motor axis 0  Motor axis 1  Motor axis 2\n",
      "0  tomo_003acc           179           245           761\n",
      "1  tomo_00e047           163           546           699\n",
      "2  tomo_01a877           147           638           286\n",
      "\n",
      "Total execution time: 189.79 seconds (3.16 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Run the submission pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Time entire process\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Generate submission\n",
    "    submission = generate_submission()\n",
    "    \n",
    "    # Print total execution time\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\nTotal execution time: {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ee02a2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T13:24:02.496626Z",
     "iopub.status.busy": "2025-06-04T13:24:02.496394Z",
     "iopub.status.idle": "2025-06-04T13:24:02.499226Z",
     "shell.execute_reply": "2025-06-04T13:24:02.498567Z"
    },
    "papermill": {
     "duration": 0.025361,
     "end_time": "2025-06-04T13:24:02.500447",
     "exception": false,
     "start_time": "2025-06-04T13:24:02.475086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tomo_00e047\t169\t546\t603\n",
    "# tomo_01a877\t147\t638\t286"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11294684,
     "sourceId": 91249,
     "sourceType": "competition"
    },
    {
     "datasetId": 7501747,
     "sourceId": 11932060,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 295634,
     "modelInstanceId": 274744,
     "sourceId": 327336,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 210.563391,
   "end_time": "2025-06-04T13:24:04.142025",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-04T13:20:33.578634",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
